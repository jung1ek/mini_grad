{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e0c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigrad import tensor\n",
    "import numpy as np\n",
    "from minigrad import nn\n",
    "from copy import deepcopy\n",
    "from minigrad.tensor import Tensor\n",
    "from typing import Callable\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3c1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used for now\n",
    "def clones(module,N):\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149ce19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self,features,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.add_parameter(\"a_2\",tensor.Tensor.ones(features))\n",
    "        self.add_parameter(\"b_2\",tensor.Tensor.zeros(features))\n",
    "    \n",
    "    def forward(self,x: Tensor):\n",
    "        return self._parameters[\"a_2\"].value*x.layer_norm(eps=self.eps)+self._parameters[\"b_2\"].value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8349e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor.training = True\n",
    "class SublayerConnection(nn.Module):\n",
    "\n",
    "    def __init__(self,size,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.add_module(\"layer_norm\",LayerNorm(features=size))\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self,x: Tensor,sublayer: Callable[[Tensor],Tensor]):\n",
    "        return x + sublayer(self._modules[\"layer_norm\"].forward(x)).dropout(p=self.dropout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f2d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query,key,value,dropout=None,mask=None):\n",
    "    dk = query.shape[-1]\n",
    "    scores = query.matmul(key.transpose(-2,-1))/math.sqrt(dk)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill_(mask.data==0,-1e9)\n",
    "    p_attn = scores.softmax(-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = p_attn.dropout(dropout)\n",
    "    return p_attn.matmul(value),p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f77c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self,in_features,out_features):\n",
    "        super().__init__()\n",
    "        self.add_parameter(\"ln_w\",Tensor.randn(in_features,out_features))\n",
    "    \n",
    "    def forward(self,x: Tensor):\n",
    "        return x.linear(self._parameters[\"ln_w\"].value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d992e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,h,dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model//h\n",
    "        self.h = h\n",
    "        self.add_module(\"ln_q\",Linear(d_model,d_model))\n",
    "        self.add_module(\"ln_k\",Linear(d_model,d_model))\n",
    "        self.add_module(\"ln_v\",Linear(d_model,d_model))\n",
    "        self.add_module(\"ln_o\",Linear(d_model,d_model))\n",
    "        self.attn = None\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.shape[0]\n",
    "        # TODO what if seqlen is different for enc and dec\n",
    "        # in enc; it is source seq len, in dec; it is target seq len.\n",
    "        seq_len = query.shape[1]\n",
    "\n",
    "        # do all linear proj in batch from d_model => h * d_k\n",
    "        query, key, value = [\n",
    "            lin.forward(x).view(nbatches, x.size(1), self.h,self.d_k).transpose(1,2)\n",
    "            for lin, x in zip((self._modules[\"ln_q\"],self._modules[\"ln_k\"],self._modules[\"ln_v\"]), (query,key,value))\n",
    "        ]\n",
    "\n",
    "        # # apply attention on all projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query,key,value,mask=mask,dropout=self.dropout\n",
    "        )\n",
    "        x = x.squeeze()\n",
    "        # concat using view and apply final linear.\n",
    "\n",
    "        x = (\n",
    "            x.transpose(1,2)\n",
    "            .view(nbatches,seq_len,self.h*self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "\n",
    "        return self._modules[\"ln_o\"].forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0dec7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.add_module(\"w1\",Linear(d_model,d_ff))\n",
    "        self.add_module(\"w2\",Linear(d_ff,d_model))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self._modules[\"w2\"].forward((self._modules[\"w1\"].forward(x).relu()).dropout(self.dropout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05525760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using explicit loop and concat for now \n",
    "class Embeddings(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,vocab):\n",
    "        super(Embeddings,self).__init__()\n",
    "        self.add_parameter(\"embed\",Tensor.randn(vocab,d_model))\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch, seq_len = x.shape\n",
    "\n",
    "        batch_out = None  # will become (batch, seq_len, d_model)\n",
    "\n",
    "        for b in range(batch):\n",
    "            seq_out = None  # will become (seq_len, d_model)\n",
    "\n",
    "            for s in range(seq_len):\n",
    "                idx = int(x[b][s].data.item())\n",
    "                vec = self._parameters[\"embed\"].value[idx].unsqueeze(0)\n",
    "                # vec shape: (1, d_model)\n",
    "                if seq_out is None:\n",
    "                    seq_out = vec\n",
    "                else:\n",
    "                    seq_out = seq_out.cat(vec, dim=0)\n",
    "            # seq_out shape: (seq_len, d_model)\n",
    "            seq_out = seq_out.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "            if batch_out is None:\n",
    "                batch_out = seq_out\n",
    "            else:\n",
    "                batch_out = batch_out.cat(seq_out, dim=0)\n",
    "        return batch_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669929e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using numpy\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,dropout,max_len=5000):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # compute the positional encodings once in log space\n",
    "        pe = np.zeros((max_len,d_model))\n",
    "        position = np.arange(0,max_len).reshape(max_len,1)\n",
    "        div_term = np.exp(\n",
    "            np.arange(0,d_model,2) * -(math.log(10000.0)/d_model)\n",
    "        )\n",
    "        pe[:,0::2] = np.sin(position*div_term)\n",
    "        pe[:,1::2] = np.cos(position*div_term)\n",
    "\n",
    "        self.pe = pe.reshape(1,max_len,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.pe[:,: x.size(1)]\n",
    "        return x.squeeze(0).dropout(self.dropout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3091a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,size,self_attn,feed_forward,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.add_module(\"ffn\",feed_forward)\n",
    "        self.add_module(\"sc1\",SublayerConnection(size,dropout))\n",
    "        self.add_module(\"sc2\",SublayerConnection(size,dropout))\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        x = self._modules[\"sc1\"].forward(x,lambda x: self.self_attn.forward(x,x,x,mask))\n",
    "        return self._modules[\"sc2\"].forward(x,self._modules[\"ffn\"].forward)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,size,self_attn,src_attn,ffn,dropout):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.add_module(\"ffn\",ffn)\n",
    "        for i in range(3):\n",
    "            self.add_module(f\"sc{i+1}\",SublayerConnection(size,dropout))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self,x,m,src_mask,tgt_mask):\n",
    "        x = self._modules[\"sc1\"].forward(x,lambda x: self.self_attn.forward(x,x,x,tgt_mask))\n",
    "        x = self._modules[\"sc2\"].forward(x,lambda x: self.src_attn.forward(x,m,m,src_mask))\n",
    "        return self._modules[\"sc3\"].forward(x,self._modules[\"ffn\"].forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e02cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        [self.add_module(f\"l{i+1}\",deepcopy(layer)) for i in range(N)]\n",
    "        self.add_module(\"ln\",LayerNorm(layer.size))\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        for module in self._modules.values():\n",
    "            if isinstance(module,EncoderLayer):\n",
    "                x = module.forward(x,mask)\n",
    "            else:\n",
    "                x = module.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,layer,N):\n",
    "        super().__init__()\n",
    "        [self.add_module(f\"l{i+1}\",deepcopy(layer)) for i in range(N)]\n",
    "        self.add_module(\"ln\",LayerNorm(layer.size))\n",
    "\n",
    "    def forward(self,x,m,src_mask,tgt_mask):\n",
    "        for module in self._modules.values():\n",
    "            if isinstance(module,DecoderLayer):\n",
    "                x = module.forward(x,m,src_mask,tgt_mask)\n",
    "            else:\n",
    "                x = module.forward(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4dbc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self,encoder,decoder,src_embed,tgt_embed,pos_src,pos_tgt,generator):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.add_module(\"enc\",encoder)\n",
    "        self.add_module(\"dec\",decoder)\n",
    "        self.add_module(\"src_emb\", src_embed)\n",
    "        self.add_module(\"tgt_emb\", tgt_embed)\n",
    "        self.pos_src = pos_src.forward\n",
    "        self.pos_tgt = pos_tgt.forward\n",
    "        self.add_module(\"gen\",generator)\n",
    "\n",
    "    def forward(self,src,tgt,src_mask,tgt_mask):\n",
    "        return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
    "    \n",
    "    def encode(self,src,src_mask):\n",
    "        return self._modules[\"enc\"].forward(self.pos_src(self._modules[\"src_emb\"].forward(src)),src_mask)\n",
    "\n",
    "    def decode(self,memeory,src_mask,tgt,tgt_mask):\n",
    "        return self._modules[\"dec\"].forward(self.pos_tgt(self._modules[\"tgt_emb\"].forward(tgt)),memeory,src_mask,tgt_mask)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,vocab):\n",
    "        super(Generator,self).__init__()\n",
    "        self.add_module(\"proj\",Linear(d_model,vocab))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self._modules[\"proj\"].forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64d1addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "        src_vocab, tgt_vocab, N=6, d_model=512, d_ff = 2048, h=8, dropout=0.1\n",
    "):\n",
    "    c = deepcopy\n",
    "    attn = MultiHeadAttention(d_model,h)\n",
    "    ff = PositionwiseFeedForward(d_model,d_ff,dropout)\n",
    "    position = PositionalEncoding(d_model=d_model,dropout=dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n",
    "        Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n",
    "        Embeddings(d_model,src_vocab),\n",
    "        Embeddings(d_model,tgt_vocab),\n",
    "        c(position),c(position),\n",
    "        Generator(d_model,tgt_vocab)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e56e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using numpy\n",
    "def subsequent_mask(size):\n",
    "    \"mask out subsequent positions.\"\n",
    "    attn_shape = (1,size,size)\n",
    "    subsequent_mask = np.tril(np.ones(attn_shape),k=0).astype(np.int8)\n",
    "    return subsequent_mask\n",
    "\n",
    "class Batch:\n",
    "    \n",
    "    def __init__(self,src,tgt=None,pad=1):\n",
    "        self.src = src\n",
    "        self.src_mask = (self.src != pad).astype(np.int8)\n",
    "        self.src_mask = self.src_mask.reshape(self.src_mask.shape[0],1,self.src_mask.shape[1])\n",
    "\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:,:-1]\n",
    "            self.tgt_y = tgt[:,1:]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).sum()\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt,pad):\n",
    "        \"create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad)\n",
    "        tgt_mask = tgt_mask.reshape(tgt_mask.shape[0],1,tgt_mask.shape[1])\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.shape[-1]).astype(\n",
    "            np.int8\n",
    "        )\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eed840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"generate random data for a src-tgt copy task\"\n",
    "    for i in range(nbatches):\n",
    "        data = np.random.randint(1,V, size=(batch_size,V))\n",
    "        data[:,0] = 1\n",
    "        src = data\n",
    "        tgt = data\n",
    "        yield Batch(src,tgt,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c5b55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self,input,target):\n",
    "        input = input.softmax(-1)\n",
    "        loss = -(target * input.log())\n",
    "        return loss.sum()/(input.shape[1]*input.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd5c4c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  18.33117\n",
      "Loss:  17.49627\n",
      "Loss:  14.991483\n",
      "Loss:  13.273812\n",
      "Loss:  13.518925\n",
      "Loss:  12.615138\n",
      "Loss:  10.464443\n",
      "Loss:  10.94338\n",
      "Loss:  10.380503\n",
      "Loss:  10.308905\n",
      "Loss:  10.114498\n",
      "Loss:  10.096762\n",
      "Loss:  9.268982\n",
      "Loss:  9.2127285\n",
      "Loss:  8.756202\n",
      "Loss:  9.260834\n",
      "Loss:  9.851846\n",
      "Loss:  8.042778\n",
      "Loss:  8.754878\n",
      "Loss:  7.9792795\n"
     ]
    }
   ],
   "source": [
    "vocab=11\n",
    "batch_size=40\n",
    "nbatches = 20\n",
    "criterion = CrossEntropyLoss()\n",
    "test_model = make_model(vocab,vocab,2,d_model=128,h=4,d_ff=256)\n",
    "data_iter = data_gen(vocab,batch_size,nbatches)\n",
    "for i,batch in enumerate(data_iter):\n",
    "    src,tgt,src_mask,tgt_mask = Tensor(batch.src), Tensor(batch.tgt), Tensor(batch.src_mask),Tensor(batch.tgt_mask)\n",
    "    out = test_model.forward(src,tgt,src_mask,tgt_mask)\n",
    "    logits = test_model._modules[\"gen\"].forward(out)\n",
    "    target = np.eye(vocab)[batch.tgt_y]\n",
    "    loss = criterion(logits,Tensor(target,requires_grad=False))\n",
    "    print(\"Loss: \",loss.data[0])\n",
    "    loss.backward()\n",
    "    for param in test_model.parameters():\n",
    "        param.update(Tensor(param.value.data-0.1 * param.value.grad.data,requires_grad=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76a721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".minigrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
